import os
import requests
import zipfile
import xml.etree.ElementTree as ET
import pandas as pd
import schedule
import time
from datetime import datetime

# Set parameters
download_urls = [
    "https://data.gov.au/data/dataset/5bd7fcab-e315-42cb-8daf-50b7efc2027e/resource/0ae4d427-6fa8-4d40-8e76-c6909b5a071b/download/public_split_1_10.zip",  # Replace with the first URL
    "https://data.gov.au/data/dataset/5bd7fcab-e315-42cb-8daf-50b7efc2027e/resource/635fcb95-7864-4509-9fa7-a62a6e32b62d/download/public_split_11_20.zip"   # Replace with the second URL
]
download_folder = "bulk_extracts"  # Folder to store downloaded zip files
extract_folder = "extracted_data"  # Folder to store extracted XML files
database_file = "abn_database.csv"  # Offline database file

# Create necessary directories
os.makedirs(download_folder, exist_ok=True)
os.makedirs(extract_folder, exist_ok=True)

# Function to download bulk extract files
def download_bulk_extract():
    print(f"Starting file download - {datetime.now()}")
    for url in download_urls:
        file_name = os.path.join(download_folder, url.split("/")[-1])
        response = requests.get(url, stream=True)
        with open(file_name, "wb") as file:
            for chunk in response.iter_content(chunk_size=1024):
                if chunk:
                    file.write(chunk)
        print(f"Downloaded: {file_name}")
    print("File download completed")

# Function to extract zip files
def extract_files():
    print(f"Starting file extraction - {datetime.now()}")
    for file_name in os.listdir(download_folder):
        if file_name.endswith(".zip"):
            zip_path = os.path.join(download_folder, file_name)
            with zipfile.ZipFile(zip_path, "r") as zip_ref:
                zip_ref.extractall(extract_folder)
            print(f"Extracted: {file_name}")
    print("Extraction completed")

# Function to parse XML files and update the database
def update_database():
    print(f"Starting database update - {datetime.now()}")
    records = []

    for file_name in os.listdir(extract_folder):
        if file_name.endswith(".xml"):
            file_path = os.path.join(extract_folder, file_name)
            context = ET.iterparse(file_path, events=("start", "end"))
            context = iter(context)

            for event, elem in context:
                if event == "end" and elem.tag == "ABR":  # Assuming each record is <ABR>
                    record = {
                        "ABN": elem.findtext("ABN"),
                        "EntityType": elem.findtext("EntityType/EntityTypeText"),
                        "State": elem.findtext("BusinessAddress/AddressDetails/State"),
                        "Postcode": elem.findtext("BusinessAddress/AddressDetails/Postcode"),
                        "GSTStatus": elem.find("GST").get("status") if elem.find("GST") is not None else None,
                        "GSTStatusFromDate": elem.find("GST").get("GSTStatusFromDate") if elem.find("GST") is not None else None,
                    }
                    records.append(record)
                    elem.clear()

    # Save results to CSV
    df = pd.DataFrame(records)
    df.to_csv(database_file, index=False)
    print("Database update completed")

# Function to query ABN
def query_abn(abn):
    if not os.path.exists(database_file):
        print("Database does not exist. Please update the database first.")
        return
    df = pd.read_csv(database_file)
    result = df[df["ABN"] == abn]
    if result.empty:
        print(f"No data found for ABN: {abn}")
    else:
        print(result)

# Function to automate tasks
def scheduled_task():
    download_bulk_extract()
    extract_files()
    update_database()

# Schedule weekly task
schedule.every().week.at("02:00").do(scheduled_task)  # Run every week at 2:00 AM

# Main loop
if __name__ == "__main__":
    print("Offline ABN Query System Started")
    while True:
        schedule.run_pending()
        time.sleep(1)
